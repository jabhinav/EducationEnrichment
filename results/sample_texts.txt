TEXT1: From the perspective of engineering, computer vision seeks to automate tasks that the human visual system can do. Areas of artificial intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot. Artificial intelligence and computer vision share other topics such as pattern recognition and learning techniques. Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general. The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of the recognition problem are described in the literature like object detection. The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover.

TEXT2: A natural question, given the wide use of Nash equilibrium, is whether or why one should expect Nash behavior. One justification is that rational players ought somehow to reason their way to Nash strategies.  That is, Nash equilibrium might arrive through introspection. A second justificationis that Nash equilibria are self-enforcing.  If players agree on a strategy profile before independently choosing their actions, then no player will have reason to deviate if the agreed profile is a Nash equilibrium. On the other hand, if the agreed profile is not a Nash equilibrium, some player can do better by breaking the agreement. A third, and final, justification is that Nash behavior might result from learning or evolution. In what follows, we take up these three ideas in turn.

TEXT3: The distinctive feature of an oligopoly is interdependence. Oligopolies are typically composed of a few large firms. Each firm is so large that its actions affect market conditions. Therefore, the competing firms will be aware of a firm's market actions and will respond appropriately. This means that in contemplating a market action, a firm must take into consideration the possible reactions of all competing firms and the firm's countermoves. It is very much like a game of chess, in which a player must anticipate a whole sequence of moves and countermoves in order to determine how to achieve his or her objectives, this is known as game theory. For example, an oligopoly considering a price reduction may wish to estimate the likelihood that competing firms would also lower their prices and possibly trigger a ruinous price war. Or if the firm is considering a price increase, it may want to know whether other firms will also increase prices or hold existing prices constant. This anticipation leads to price rigidity as firms will be only be willing to adjust their prices and quantity of output in accordance with a "price leader" in the market. This high degree of interdependence and need to be aware of what other firms are doing or might do is to be contrasted with lack of interdependence in other market structures. In a perfectly competitive (PC) market there is zero interdependence because no firm is large enough to affect market price. All firms in a PC market are price takers, as current market selling price can be followed predictably to maximize short-term profits. In a monopoly, there are no competitors to be concerned about. In a monopolistically-competitive market, each firm's effects on market conditions is so negligible as to be safely ignored by competitors.

TEXT4: Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, partially supervised or unsupervised. Some representations are loosely based on interpretation of information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain. Research attempts to create efficient systems to learn these representations from large-scale, unlabeled data sets. Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation and bioinformatics where they produced results comparable to and in some cases superior to human experts.

TEXT5: Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century. Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals. There is not even consensus on whether mathematics is an art or a science. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. Some just say, "Mathematics is what mathematicians do". Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory. Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity.

TEXT6: Economics (UK English: /iːkəˈnɒmɪks/, /ɛkəˈnɒmɪks/;[1] US English: /ɛkəˈnɑːmɪks/, /ikəˈnɑːmɪks/[2][3]) is a social science concerned chiefly with description and analysis of the production, distribution, and consumption of goods and services. Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyzes basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the entire economy (meaning aggregated production, consumption, savings, and investment) and issues affecting it, including unemployment of resources (labour, capital, and land), inflation, economic growth, and the public policies that address these issues (monetary, fiscal, and other policies). Other broad distinctions within economics include those between positive economics, describing "what is", and normative economics, advocating "what ought to be"; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.

TEXT7: Applied Economics is the application of economic theory and econometrics in specific settings. As one of the two sets of fields of economics (the other set being the core), it is typically characterized by the application of the core, i.e. economic theory and econometrics, to address practical issues in a range of fields including demographic economics, labour economics, business economics, industrial organization, agricultural economics, development economics, education economics, health economics, monetary economics, public economics, and economic history. The process often involves a reduction in the level of abstraction of this core theory. There are a variety of approaches including not only empirical estimation using econometrics, input-output analysis or simulations but also case studies, historical analogy and so-called common sense or the "vernacular". This range of approaches is indicative of what Roger Backhouse and Jeff Biddle argue is the ambiguous nature of the concept of applied economics. It is a concept with multiple meanings. Among broad methodological distinctions, one source places it in neither positive nor normative economics but the art of economics, glossed as "what most economists do".

TEXT8: Risk tolerance is a crucial factor in personal financial decision making. Risk tolerance is defined as individuals' willingness to engage in a financial activity whose outcome is uncertain. Behavioral economics is primarily concerned with the bounds of rationality of economic agents. Behavioral models typically integrate insights from psychology, neuroscience and microeconomic theory; in so doing, these behavioral models cover a range of concepts, methods, and fields. The study of behavioral economics includes how market decisions are made and the mechanisms that drive public choice. The use of the term "behavioral economics" in U.S. scholarly papers has increased in the past few years, as shown by a recent study. Behavioral economics has also been applied to intertemporal choice. Intertemporal choice is defined as making a decision and having the effects of such decision happening in a different time. 

TEXT9: Neuroscience (or neurobiology) is the scientific study of the nervous system. Neuroscience is a multidisciplinary branch of biology, that deals with the anatomy, biochemistry, molecular biology, and physiology of neurons and neural circuits. Biochemistry, sometimes called biological chemistry, is the study of chemical processes within and relating to living organisms. Neuroscience also draws upon other fields, with the most obvious being pharmacology, psychology, and medicine. The scope of neuroscience has broadened over time to include different approaches used to study the molecular, cellular, developmental, structural, functional, evolutionary, computational, psychosocial and medical aspects of the nervous system. Neuroscience has also given rise to such other disciplines as neuroeducation, neuroethics, and neurolaw. The techniques used by neuroscientists have also expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory and motor tasks in the brain.

TEXT10: Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. Boltzmann machines were one of the first neural networks capable of learning internal representations, and are able to represent and (given sufficient time) solve difficult combinatoric problems. A Boltzmann machine, like a Hopfield network, is a network of units with an "energy" defined for the overall network.

TEXT11: Chemistry is the scientific discipline involved with compounds composed of atoms, i.e. elements, and molecules, i.e. combinations of atoms: their composition, structure, properties, behavior and the changes they undergo during a reaction with other compounds. Chemistry addresses topics such as how atoms and molecules interact via chemical bonds to form new chemical compounds. There are four types of chemical bonds: covalent bonds, in which compounds share one or more electron(s); ionic bonds, in which a compound donates one or more electrons to another compound to produce ions: cations and anions; hydrogen bonds; and Van der Waals force bonds.

TEXT12: Support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. When data are not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data are not labeled or when only some data are labeled as a preprocessing for a classification pass. Support vector machines are helpful in text and hypertext categorization as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings. The support vector machines has been widely applied in the biological and other sciences. Hand written characters can be recognized using support vector machines. Classification of images can also be performed using support vector machines. Experimental results show that support vector machines achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. 

TEXT13: Physics "knowledge of nature", is the natural science that involves the study of matter and its motion and behavior through space and time, along with related concepts such as energy and force. One of the most fundamental scientific disciplines, the main goal of physics is to understand how the universe behaves. Physics is one of the oldest academic disciplines, perhaps the oldest through its inclusion of astronomy. Over the last two millennia, physics was a part of natural philosophy along with chemistry, biology, and certain branches of mathematics, but during the scientific revolution in the 17th century, the natural sciences emerged as unique research programs in their own right.

TEXT14: Botany is the scientific study of plants. "Plants," to most people, means a wide range of living organisms from the smallest bacteria to the largest living things - the giant sequoia trees. By this definition plants include: algae, fungi, lichens, mosses, ferns, conifers and flowering plants. Today scientists believe bacteria, algae and fungi are in their own distinct kingdoms, but most general botany courses, and most Botany Departments at colleges and universities, still teach about these groups. Because the field is so broad, there are many kinds of plant biologists and many different opportunities available. Botanists interested in ecology study interactions of plants with other organisms and the environment.
